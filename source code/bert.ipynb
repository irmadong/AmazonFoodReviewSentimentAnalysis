{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKcfvhiB_OCJ",
        "outputId": "b0e2792e-c766-4399-e8d9-3b1f1116e2db"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.5MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30537 sha256=1faf10f55d2072486d8098c45f07d09ef26fda2de2ddad439836525d98301e77\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=d7a04552b6950aaf5ca18daa09ba5d6084f13650b24292e5aa2a4dbbf6dbfe30\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19475 sha256=44b114e33e5ea8fb6f2d7b119277fdbdabc476ad22d653313c1927313f5b6bce\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_4yyeYx_RYV",
        "outputId": "27be4a6f-1df8-42c0-bed6-bf39d63250a7"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 5.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVDCrQ27_Yxi"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g7y3KaXqgPe"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MT8CQ84xRIE"
      },
      "source": [
        "Data Procees and Pretrain Bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8UxfAo8brgl"
      },
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "class dataprocessor:\n",
        "\n",
        "    def __init__(self,filename='./Reviews_new.csv',neutral=0):\n",
        "        self.df_original = pd.read_csv(filename,header=1,encoding='utf-8',engine=\"python\",error_bad_lines=False)\n",
        "        self.df_original=self.df_original.dropna()\n",
        "        self.df_base = self.data_process_baseline(self.df_original,neutral)\n",
        "\n",
        "\n",
        "    def data_process_baseline(self,df_original, neutral):\n",
        "\n",
        "        senti = []\n",
        "        summary = []\n",
        "        review = []\n",
        "        for i, j in df_original.iterrows():\n",
        "            helpfulness = 0.0\n",
        "            if int(j[5]) > 0:\n",
        "                helpfulness = float(int(j[4])/int(j[5]))\n",
        "            if helpfulness > 0.0:\n",
        "                if int(j[6]) > 3:\n",
        "                    senti.append(1)\n",
        "                    #summary.append(self.tokenize_clean(j[8]))\n",
        "                    review.append(self.tokenize_clean(j[9]))\n",
        "                if int(j[6]) < 3:\n",
        "                    senti.append(0)\n",
        "                    #summary.append(self.tokenize_clean(j[8]))\n",
        "                    review.append(self.tokenize_clean(j[9]))\n",
        "\n",
        "                #consider neutral cases\n",
        "                if neutral:\n",
        "                    if int(j[6]) == 3:\n",
        "                        senti.append(0)\n",
        "                        #summary.append(self.tokenize_clean(j[8]))\n",
        "                        review.append(self.tokenize_clean(j[9]))\n",
        "        data_baseline = {\"Sentiment\": senti, \"Review\": review}\n",
        "        df_baseline = pd.DataFrame(data_baseline, columns=[\"Sentiment\",\"Review\"])\n",
        "        return df_baseline\n",
        "    def tokenize_clean(self,sent):\n",
        "\n",
        "      sentence = remove_tags(sent)\n",
        "\n",
        "      # Remove punctuations and numbers\n",
        "      sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "      # Single character removal\n",
        "      sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "      # Removing multiple spaces\n",
        "      sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "      return sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3t4aUL_isT"
      },
      "source": [
        " def remove_tags(text):\n",
        "      return TAG_RE.sub('', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7H4b-MjbxjL"
      },
      "source": [
        "\n",
        "source_data=dataprocessor(\"Reviews_new.csv\").df_base\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QRpXK7f_2Ak"
      },
      "source": [
        "\n",
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkEDcPf-EByC"
      },
      "source": [
        "def tokenize_reviews(text_reviews):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av9MpJ_AEGmT"
      },
      "source": [
        "reviews = source_data['Review'].tolist()\n",
        "y = source_data['Sentiment']\n",
        "tokenized_reviews = [tokenize_reviews(review) for review in reviews]\n",
        "reviews_with_len = [[review, y[i], len(review)]\n",
        "                 for i, review in enumerate(tokenized_reviews)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrkL2xbcGMy5"
      },
      "source": [
        "import random\n",
        "random.shuffle(reviews_with_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuSE-gWoHotY"
      },
      "source": [
        "reviews_with_len.sort(key=lambda x: x[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiUkzk5ZHsHl"
      },
      "source": [
        "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQnnlCqdHvxM"
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVll6Zb2xYfR"
      },
      "source": [
        "Batch & Create Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUDhDVA6HxtK"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKdySoj3H6aV"
      },
      "source": [
        "import math\n",
        "\n",
        "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
        "TEST_BATCHES = TOTAL_BATCHES // 20\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZOxp6SIxdRB"
      },
      "source": [
        "Create CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3v49cfoLVDu"
      },
      "source": [
        "class TEXT_MODEL(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"text_model\"):\n",
        "        super(TEXT_MODEL, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size,\n",
        "                                          embedding_dimensions)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if model_output_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        l_3 = self.cnn_layer3(l)\n",
        "        l_3 = self.pool(l_3) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCOjXcWwMeTN"
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 256\n",
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2xBE6GNMflY"
      },
      "source": [
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
        "                        embedding_dimensions=EMB_DIM,\n",
        "                        cnn_filters=CNN_FILTERS,\n",
        "                        dnn_units=DNN_UNITS,\n",
        "                        model_output_classes=OUTPUT_CLASSES,\n",
        "                        dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z719ge7sMhW2"
      },
      "source": [
        "if OUTPUT_CLASSES == 2:\n",
        "    text_model.compile(loss=\"binary_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "else:\n",
        "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N_8u8XXxipM"
      },
      "source": [
        "Apply Bert with Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGbgVEfZMjFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da425eb-1386-4764-82ae-f2fd6aab57a6"
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "627/627 [==============================] - 101s 161ms/step - loss: 0.2980 - accuracy: 0.8754\n",
            "Epoch 2/5\n",
            "627/627 [==============================] - 101s 161ms/step - loss: 0.1122 - accuracy: 0.9584\n",
            "Epoch 3/5\n",
            "627/627 [==============================] - 102s 162ms/step - loss: 0.0591 - accuracy: 0.9802\n",
            "Epoch 4/5\n",
            "627/627 [==============================] - 104s 165ms/step - loss: 0.0239 - accuracy: 0.9924\n",
            "Epoch 5/5\n",
            "627/627 [==============================] - 103s 164ms/step - loss: 0.0091 - accuracy: 0.9974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0ec9d22358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCuLfqpKxh3M"
      },
      "source": [
        "Evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLG7jYUyMm7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1176a5b1-da61-46f4-8cb8-69fca715a1d3"
      },
      "source": [
        "results = text_model.evaluate(test_data)\n",
        "print(results)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 11ms/step - loss: 0.2542 - accuracy: 0.9287\n",
            "[0.25419896841049194, 0.9287109375]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}